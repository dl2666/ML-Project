# ML-Project

WHY FAKE NEWS IS A PROBLEM?

With the politically heated climate that we live in today, stopping the spread of misinformation is incredibly important. Too easily can fake news spread throughout social networks, that it builds an inherent mistrust in society itself and furthers the divide between political factions. This is why creating machine learning algorithms that can quickly identify false news reports is of great importance because it allows for swift action by whichever platform is hosting the article to provide a false label or removing it entirely from the site.

THE DATASET:

	The dataset I used consists of about 40,000 fake and real news articles, provided from two separate datasets originally (one real news articles only and the other fake articles only). The data itself has five features for each article: title, text, subject, date, and target – 0 for real, 1 for fake. The goal is to train a model off the labeled data and learn to correctly classify fake vs. real news. 

THE MODEL:

For my baseline model, I plan to train a two-layer LSTM model using Word2Vec for the embedding layer.  As neural networks are unable to read plain text as an input, NLP models include an embedding layer that vectorizes sentences into machine readable integers. One of the most popular methods is Word2Vec embedding, which allows for word meaning not to be lost during the conversion process. Words get their embeddings by looking at which other words they tend to appear next to in documents/articles/books/etc. The input to the Word2Vec model is a token/word, and then the model outputs a prediction vector with a probability assigned to each word in its vocabulary based on the inputted word

A LSTM network is an improved upon version of a RNN network with the addition of memory gates. LSTM models are often used for natural language processing because the model can remember or forget previous learnings more selectively. This is important when choosing word meaning, as previous learned text sequences can be recovered when learning the current word. Compared to RNNs, which only have short-term memory, when used in combination with Long Short-Term Memory (LSTM) Gates, the network can have long term memory. Instead of the recurring section of an RNN, an LTSM is a small neural network consisting of four neural network layers. These are the recurring layer from the RNN with three networks acting as gates.
LSTM has three gates:
•	An Input gate, this controls the information input at each time step.
•	An Output gate, this controls how much information is outputted to the next cell or upward layer
•	A Forget gate, this controls how much data to lose at each time step.
This cell state is in turn what allows for long-term memory to exist 

PREPROCESSING THE DATA:

Before the text can be embedded into word vectors, it needs to be ‘cleaned’ and processed so only the most important words will be inputted to the neural network. Some examples include removing stopwords, lemmatization, and tokenization. Stopwords are the English words which do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the text. For example, words like the, she, have etc. Lemmatization performs vocabulary and morphological analysis of the word and is normally aimed at removing inflectional endings only. An inflectional ending is a group of letters added to the end of a word to change its meaning. Some inflectional endings are: -s. bat. bats. Tokenization is the process of breaking down a text into words. Tokenization can happen on any character; however, the most common way of tokenization is to do it on the space character.


EXPLORATORY DATA ANALYSIS:

Upon taking a closer look at the text data itself, it appears the most common words among both real and fake datasets differ slightly. Fake news articles include ‘trump’ at almost 2x the rate compared to real articles. While real news articles have ‘said’ as the most often used word at around, appearing around 70,000 times, with the runner-up being ‘U.S’ at around 40,000 times. Looking at the distribution of words for both types of articles, it appears most of the fake articles are between 0 to 500 words while real articles have a majority from 0 to 250 words. In regard to the publishing dates of both datasets, the number of fake articles released per day remained consistently higher than the real articles, except for notable spikes on November 9, 2016 and from September 2017 to December 2017. The latter spike is most likely due to federal elections for State Senators and Representatives occurring during this time period. Lastly, I removed any publisher names that were included in the ‘text’ column of my dataset as none of the fake articles listed a publisher, and therefore, an easy indicator to identify a fake or real article without analyzing the actual text. The subject column also wasn’t used for the actual training for the same reason, as real news articles only consisted of two out of the six subject categories, ‘policticsnews’ and ‘worldnews’. 


TRAINING THE LSTM MODEL:

	The model trained on 23,234 samples and validated on 9958 samples. All samples were set to a maximum length of 700 words and anything shorter would be pre-padded with 0 for all indices with no corresponding words. The word2vec embedding layer was trained to include words that appeared 5 or more times in its word index, with a window size of 5. This was fed into the 2-layer LSTM model, which included a dropout rate of 0.2 and a dense layer with sigmoid activation.  After 10 epochs with batch sizes of 256, the model reached an accuracy score of 99%. The model attained very high accuracy by the 2nd to 3rd epochs and continued to increase from .92 to .9864. I believe the model was able to reach .99 accuracy due to the fact that the most common words between the two datasets differed and for the words they did share in common, they would have vastly different frequencies. Therefore, an LSTM model, which trains on word sequences, can easily identify a fake report from a real one just based on the words themselves.
	As the dataset appeared to have many distinguishing words within the ‘text’ column that easily differentiated real vs fake articles, I decided to train a new LSTM model on just the ‘title’ column instead of both ‘title’ and ‘text’ columns to see if overfitting would occur again. Both datasets had ‘trump’ as the most frequent word in titles and at similar rates, which fixed the earlier issue of easily identifying fake articles by the name ‘trump’. After 12 epochs of training the new LSTM model on just article titles, my model reached an overall accuracy of 90%. This model again was better at predicting whether an article was fake with a precision of .95, while precision for real articles was at .86. On the hand, the percentage of positive cases the model caught – its recall, was higher for real articles at .95 while the recall for fake articles was .85 
	In addition to the two LSTM models I trained and after the Q & A session, it was recommended I try Naïve Bayes or Logistic Regression models to train the dataset after expressing concerns over the text. I used Count Vectorizer to transform the title columns into word vectors and trained both a Multinomial Naïve Bayes model and a Logistic Regression model and got accuracy scores of .94 and .95 respectively.	

CONCLUSIONS:

	After training 4 models on the Real vs. Fake news dataset, it has become very evident to me that before choosing the type of model to train on, you need to have a clear understanding on what’s included in the text. This is because there may be features that are not included in both classes that allow for easy prediction without actually ‘training’ on the data. Making sure both datasets are mostly equivalent in all features is key to training a model correctly. If there are major differences in the class data, like real articles being half the length of fake articles, large differences in common words, missing values, etc., the model may just be memorizing the data and it won’t translate well to other datasets. 
